{"cells":[{"cell_type":"markdown","metadata":{"id":"MB-CMX_sx7Ox"},"source":["## Author : Rahul Bhoyar"]},{"cell_type":"markdown","metadata":{"id":"dXnQagqc_J1P"},"source":["### Proof of Concept: Leveraging Language Models for the Construction of a Comprehensive Dataset"]},{"cell_type":"markdown","metadata":{"id":"5meokdBWocRK"},"source":["\n","**Objective:**\n","\n","The aim of this endeavor is to assess the viability of leveraging a Large Language Model (LLM) for the purpose of constructing a comprehensive Kaggle Dataset.\n","\n","This evaluation seeks to determine the feasibility and effectiveness of employing a Language Model in the creation of a comprehensive dataset for Kaggle.\n","\n","**Tutorial:**\n","\n","In this tutorial, we outline a systematic approach to creating extensive datasets leveraging Language Model (LLM) capabilities. Our focus is on utilizing LLMs to crawl through the Kaggle website and extract relevant data for constructing a database.\n","\n","**Steps:**\n","\n","**1. User Input:** Begin by prompting the user to provide a keyword for which the dataset is to be constructed.\n","\n","**2. URL Construction:** Utilize the provided keyword to construct a Kaggle URL, which will serve as the target for web crawling in the subsequent steps.\n","\n","**3. Web Page Loader Initialization:** Employ the *llama_index* library's web page loader to initialize the crawling process. The primary function of this loader is to navigate through the webpage, extracting information and generating a comprehensive \"document.\"\n","\n","**4. Vectorization and Database Storage:** Once the document is obtained, proceed to vectorize its contents. Store the resulting vectors in a dedicated Vector Database. This step ensures a structured and organized representation of the crawled data.\n","\n","**5. Querying with ChatGPT 4.0:** Leverage the capabilities of CHATGPT 4.0 to query the stored data in the Vector Database. Construct queries that facilitate the extraction of relevant information pertaining to Kaggle datasets from the documents.\n","\n","**6. Execution of Queries:** Execute the queries within the CHATGPT 4.0 environment to obtain specific details and insights related to Kaggle datasets present in the crawled documents. This step enhances the efficiency of data retrieval.\n","\n","By following these systematic steps, users can harness the power of LLMs to construct and query comprehensive datasets from Kaggle, facilitating streamlined access to valuable information for diverse purposes.\n"]},{"cell_type":"markdown","metadata":{"id":"ZLARwzxxytSC"},"source":["#### Step 1 -  User Input: Begin by prompting the user to provide a keyword for which the dataset is to be constructed."]},{"cell_type":"markdown","metadata":{"id":"jv-p3kpOy5of"},"source":["(A) Install the required libraries."]},{"cell_type":"code","source":["!pip install langchain langchain-openai faiss-cpu beautifulsoup4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BC35VfmEjO5L","executionInfo":{"status":"ok","timestamp":1708115374055,"user_tz":-60,"elapsed":9953,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"93baa7ce-3608-4bd8-b22a-1bb41149332d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.7)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.0.6)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.20 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.20)\n","Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.23)\n","Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.87)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.12.0)\n","Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.6.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.3)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"OfSC1ZbVzCsM"},"source":["(B) Take the user input for which we need to construct dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5368,"status":"ok","timestamp":1708115379413,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"},"user_tz":-60},"id":"8kwBIrD-zMP9","outputId":"d9f2d45d-49e8-4927-c374-51b1f4815f16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the keyword for which you need to construct the Kaggle dataset :Healthcare\n","----------------------------------------------------------------------------------------------------\n","Provided input by user is : Healthcare\n"]}],"source":["keyword_input = str(input(\"Enter the keyword for which you need to construct the Kaggle dataset :\"))\n","\n","print(\"-\"*100)\n","print(\"Provided input by user is :\", keyword_input)"]},{"cell_type":"markdown","metadata":{"id":"G2cK1HM0x6Re"},"source":["#### Step 2 - URL Construction: Utilize the provided keyword to construct a Kaggle URL, which will serve as the target for web crawling in the subsequent steps."]},{"cell_type":"markdown","metadata":{"id":"MfWA_7zx0D6v"},"source":["Construct the Kaggle URL with from above keyword."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708115379413,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"},"user_tz":-60},"id":"aYj7Nkw6z7oV","outputId":"6ec9ef40-df86-4e8e-abaf-18f7cbbc70db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Constructed Kaggle URL is : https://www.kaggle.com/search?q=Healthcare\n"]}],"source":["URL = f\"https://www.kaggle.com/search?q={keyword_input}\"\n","\n","print(\"Constructed Kaggle URL is :\",URL)"]},{"cell_type":"markdown","metadata":{"id":"j6fQPOg60SGO"},"source":["#### Step 3 - Web Page Loader Initialization: Employ the llama_index library's web page loader to initialize the crawling process. The primary function of this loader is to navigate through the webpage, extracting information and generating a comprehensive \"document.\""]},{"cell_type":"markdown","metadata":{"id":"4CwPLRAXKa8s"},"source":["(A) Setting up OPENAI's environment. As a part of setting up the environment, we will require OPENAI's API Access key.\n","\n","We will store is as our environment variable \"OPENAI_API_KEY\"."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708115379413,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"},"user_tz":-60},"id":"Adi9ZBzWKaNk","outputId":"af4aa2b4-ce38-49d9-a38d-f27914fa21d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["OPENAI API key is set successfully : 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\n"]}],"source":["import os\n","openai_api_key = \"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\"  # Enter your OPENAI_API_KEY\n","os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n","print(\"OPENAI API key is set successfully :\",openai_api_key)"]},{"cell_type":"markdown","metadata":{"id":"BnwHfn7S07_1"},"source":["(B) Crawl through the web and store the data from webpage as document."]},{"cell_type":"code","source":["from langchain_community.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(URL)\n","documents = loader.load()\n","print(\"Document object created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_26SWKkjrSm","executionInfo":{"status":"ok","timestamp":1708115381089,"user_tz":-60,"elapsed":1680,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"6bebb6db-bfbf-47dd-becc-4f1e269fd3ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Document object created.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1708115381090,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"},"user_tz":-60},"id":"j9zKVNQgGduO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05afbc57-9122-4fda-f1d6-702672474dc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Document loaded from Kaggle Webpage :\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","[Document(page_content='\\n\\n\\n\\nSearch | Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.kaggle.com/search?q=Healthcare', 'title': 'Search | Kaggle', 'description': 'Search for anything on Kaggle.', 'language': 'en'})]\n"]}],"source":["print(\"Document loaded from Kaggle Webpage :\")\n","print(\"-\"*200)\n","print(documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708115381090,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"},"user_tz":-60},"id":"G0KfVxyR14L5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae8cc967-a675-47df-e36a-949180ff3bdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["The length of the document object : 1\n"]}],"source":["print(\"The length of the document object :\", len(documents))"]},{"cell_type":"markdown","metadata":{"id":"4s2RkBaK2jXa"},"source":["#### Step 4 - Vectorization and Database Storage:\n","\n","Once the document is obtained, proceed to vectorize its contents. Store the resulting vectors in a dedicated Vector Database. This step ensures a structured and organized representation of the crawled data."]},{"cell_type":"markdown","metadata":{"id":"q30EmjRq1_8L"},"source":["(A) Initialise the LLM. We will be using GPT-4 as our base LLM model."]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","MODEL_NAME = \"gpt-4\"\n","llm = ChatOpenAI(model = MODEL_NAME)\n","print(\"LLM model loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K023Eu5ckAfI","executionInfo":{"status":"ok","timestamp":1708115382198,"user_tz":-60,"elapsed":1112,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"aa7656c1-b273-4040-f970-bb7f3176f1c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM model loaded successfully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"fYY88sw-2cQu"},"source":["(B) Initialise the embedding model which we will use to vectorise the document data and convert it into numerical representation."]},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","#MODEL_NAME = \"gpt-4\"\n","embeddings = OpenAIEmbeddings()\n","print(\"LLM model for embeddings loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsQr_RDBkPs4","executionInfo":{"status":"ok","timestamp":1708115382198,"user_tz":-60,"elapsed":3,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"056ca981-a7d1-46be-afde-3de0177a1476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM model for embeddings loaded successfully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"dlUXEDZq46EE"},"source":["(C) Creation of vector object to convert the numerical representation of documents."]},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","\n","text_splitter = RecursiveCharacterTextSplitter()\n","documents = text_splitter.split_documents(documents)\n","vector = FAISS.from_documents(documents, embeddings)\n","\n","print(\"Vector object created successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4s22jZ8qky9S","executionInfo":{"status":"ok","timestamp":1708115383085,"user_tz":-60,"elapsed":889,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"d5d7bdff-c4ca-437e-bf15-52e1f6c53605"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector object created successfully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"1lGMMZ2z5SEt"},"source":["(D) Creation of chain object."]},{"cell_type":"code","source":["from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","document_chain = create_stuff_documents_chain(llm, prompt)"],"metadata":{"id":"XKAgfuWrlpNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import create_retrieval_chain\n","\n","retriever = vector.as_retriever()\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)"],"metadata":{"id":"HvYNYAsplpKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2RaJTce6A1A"},"source":["#### Step 6 - Execution of Queries: Execute the queries within the CHATGPT 4.0 environment to obtain specific details and insights related to Kaggle datasets present in the crawled documents. This step enhances the efficiency of data retrieval."]},{"cell_type":"markdown","metadata":{"id":"e7aFxZHP5hiA"},"source":["(A) Creating a query so that we can fetch the data in the desired format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Bk--aF7HB69"},"outputs":[],"source":["query = \"\"\"\n","Give me all the Kaggle datasets form link with its description from the text that you have.\n","Give me all possible datasets.\n","Format should be like this:\n","index : Serial number of the dataset\n","(next line)\n","dataset_name : Name of the dataset\n","(next line)\n","description : Description of the dataset\n","(next line)\n","link : Link of the dataset\n","(next line)\n","\"\"\""]},{"cell_type":"code","source":["response = retrieval_chain.invoke({\"input\": query})\n","answer = response[\"answer\"]"],"metadata":{"id":"HGXQPT-GmADD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"-\"*200)\n","print(\"The answer is :\")\n","print(\"-\"*200)\n","print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9LHBZZpmQPo","executionInfo":{"status":"ok","timestamp":1708115386392,"user_tz":-60,"elapsed":3,"user":{"displayName":"Rahul Bhoyar","userId":"03458412160178633274"}},"outputId":"b43fa9d4-df86-431e-b919-369ad338bba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","The answer is :\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","The text does not provide any Kaggle datasets or their descriptions.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BcnGHCPe5rn9"},"source":["### Conclusion :"]},{"cell_type":"markdown","metadata":{"id":"622Q4GEg-H4X"},"source":["\n","The utilization of the language model (LLM) for the creation of a comprehensive dataset faces limitations, as evidenced by the incapability of crawling Kaggle URLs using the Beautiful Soup loader within llama_index.\n","\n","Consequently, the RAG-based approach proves unfeasible for constructing the Kaggle dataset.\n","\n","The hindrance arises from the inability to access Kaggle URLs through the specified Beautiful Soup loader implemented by llama_index, rendering the proposed methodology impractical for dataset generation."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}